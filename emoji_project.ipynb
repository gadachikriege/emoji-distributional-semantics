{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preformatted and semi-preprocessed dataset\n",
    "tweets_and_labels_RAW = pd.read_csv('emoji_datasets/all_data.csv', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_emojis(example):\n",
    "    result = []\n",
    "    ptr = 0\n",
    "    for i,c in enumerate(example):\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            split = example[ptr:i]\n",
    "            if split != '':\n",
    "                result.append(split)\n",
    "                result.append(c)\n",
    "            else:\n",
    "                result.append(c)\n",
    "            ptr = i+1\n",
    "    return result\n",
    "\n",
    "def preprocess(data):\n",
    "    labels = list(data[:,0])\n",
    "    tweets = list(data[:,1])\n",
    "    result = []\n",
    "    tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    for i,twt in enumerate(tweets):\n",
    "        clean_tokens = []\n",
    "        tokens = tweet_tokenizer.tokenize(twt)\n",
    "        for j,tk in enumerate(tokens):\n",
    "            tk = tk.lower()\n",
    "            sep = separate_emojis(tk)\n",
    "            if sep != []:\n",
    "                clean_tokens = clean_tokens + sep\n",
    "            else:\n",
    "                clean_tokens.append(tk)\n",
    "        result.append((labels[i], clean_tokens))\n",
    "    return result\n",
    "\n",
    "def find_all_emojis(data):\n",
    "    emoji_dict = defaultdict(int)\n",
    "    for twt in data:\n",
    "        for word in twt[1]:\n",
    "            if word in emoji.UNICODE_EMOJI:\n",
    "                emoji_dict[word] += 1\n",
    "    return emoji_dict\n",
    "\n",
    "def term_context_matrix(targets, data):\n",
    "    tc_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for twt in data:\n",
    "        for w1 in targets:\n",
    "            if w1 in twt[1]:\n",
    "                for w2 in twt[1]:\n",
    "                    tc_matrix[w1][w2]+=1\n",
    "    return tc_matrix\n",
    "\n",
    "def vocab_map(dd):\n",
    "    vocab = {}\n",
    "    vocab_id = 0\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if k2 not in vocab.keys():\n",
    "                vocab[k2] = vocab_id\n",
    "                vocab_id += 1\n",
    "    return vocab\n",
    "\n",
    "def term_to_int_dd(dd):\n",
    "    num_rows = len(dd.keys())\n",
    "    data = defaultdict(int)\n",
    "    vocab_dict = vocab_map(dd)\n",
    "    for i,r in enumerate(dd.keys()):\n",
    "        for j,c in enumerate(dd[r].keys()):\n",
    "            data[i,vocab_dict[c]] = dd[r][c]\n",
    "            \n",
    "    return data\n",
    "\n",
    "def term_to_sparse(dd):\n",
    "    dd_int = term_to_int_dd(dd)\n",
    "    vs = [v for (i,j), v in dd_int.items()]\n",
    "    ii = [i for (i,j), v in dd_int.items()]\n",
    "    jj = [j for (i,j), v in dd_int.items()]\n",
    "    matrix = coo_matrix((vs, (ii, jj)))\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_labels = preprocess(tweets_and_labels_RAW.values)\n",
    "emoji_counts = find_all_emojis(tweets_and_labels)\n",
    "emoji_targets = list(emoji_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = term_context_matrix(emoji_targets, tweets_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_coo_matrix = term_to_sparse(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_bigram_counts(targets, tweets, term_matrix):\n",
    "    target_counter = Counter()\n",
    "    vocab_counter = Counter()\n",
    "    bigram_counter = Counter()\n",
    "    for twt in tweets:\n",
    "        for word in twt[1]:\n",
    "            if word in targets:\n",
    "                target_counter[word] += 1\n",
    "            vocab_counter[word] += 1\n",
    "    \n",
    "    for emoji in term_matrix.keys():\n",
    "        for word in term_matrix[emoji].keys():\n",
    "            bigram_counter[(emoji,word)] = term_matrix[emoji][word]\n",
    "            \n",
    "    return target_counter, vocab_counter, bigram_counter\n",
    "\n",
    "\n",
    "def unigram_index_maps(counter):\n",
    "    val_to_index, index_to_val = {}, {}\n",
    "    for i, x in enumerate(counter.keys()):\n",
    "        val_to_index[x] = i\n",
    "        index_to_val[i] = x\n",
    "    return val_to_index, index_to_val\n",
    "\n",
    "\n",
    "def pmi_matrix(params):\n",
    "    '''\n",
    "        tc   : target_counts\n",
    "        vc   : vocab_counts\n",
    "        bc   : bigram_counts\n",
    "        \n",
    "        tcs  : target_count_sum\n",
    "        vcs  : vocab_count_sum\n",
    "        bcs  : bigram_count_sum\n",
    "        \n",
    "        tv2i : target_v2i  (target value to index)\n",
    "        ti2v : target_i2v  (index to target value)\n",
    "        vv2i : vocab_v2i   (vocab value to index)\n",
    "        vi2v : vocab_i2v   (index to vocab value)\n",
    "    '''\n",
    "    \n",
    "    tc   = params['tc']\n",
    "    vc   = params['vc']\n",
    "    bc   = params['bc']\n",
    "    tcs  = params['tcs']\n",
    "    vcs  = params['vcs']\n",
    "    bcs  = params['bcs']\n",
    "    tv2i = params['tv2i']\n",
    "    ti2v = params['ti2v']\n",
    "    vv2i = params['vv2i']\n",
    "    vi2v = params['vi2v']\n",
    "    \n",
    "    pmi_samples = Counter()\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for (x, y), n in bc.items():\n",
    "        rows.append(tv2i[x]) # target index\n",
    "        cols.append(vv2i[y]) # context index\n",
    "        data.append(log((n / bcs) / (((tc[x] / tcs)) * ((vc[y] / vcs)**0.75))))\n",
    "        #data.append(log( (( (n / bcs)**0.75 ) / (((tc[x] / tcs)**0.75) * (vc[y] / vcs)))**0.75 ))\n",
    "        pmi_samples[(x, y)] = data[-1]\n",
    "        \n",
    "    pmi_matrix = csc_matrix((data, (rows, cols)))\n",
    "    \n",
    "    return pmi_matrix, pmi_samples \n",
    "\n",
    "\n",
    "def prune_counts(tc_RAW, vc_RAW, bc_RAW):\n",
    "    '''\n",
    "        Remove target and vocab words which occur less than 5 times\n",
    "        Replace with UNK word\n",
    "    '''\n",
    "    tc = tc_RAW.copy()\n",
    "    vc = vc_RAW.copy()\n",
    "    bc = bc_RAW.copy()\n",
    "    \n",
    "    min_occurence = 5\n",
    "    unk = 'UNK'\n",
    "    for wt in list(tc.keys()):\n",
    "        if tc[wt] < min_occurence:\n",
    "            count = tc[wt]\n",
    "            del tc[wt]\n",
    "            tc[unk] += count\n",
    "    for wv in list(vc.keys()):\n",
    "        if vc[wv] < min_occurence:\n",
    "            count = vc[wv]\n",
    "            del vc[wv]\n",
    "            vc[unk] += count\n",
    "    for x,y in list(bc.keys()):\n",
    "        if x not in tc and y not in vc:\n",
    "            count = bc[(x, y)]\n",
    "            del bc[(x, y)]\n",
    "            bc[(unk,unk)] += count\n",
    "        elif x not in tc:\n",
    "            count = bc[(x, y)]\n",
    "            del bc[(x, y)]\n",
    "            bc[(unk,y)] += count\n",
    "        elif y not in vc:\n",
    "            count = bc[(x, y)]\n",
    "            del bc[(x, y)]\n",
    "            bc[(x,unk)] += count\n",
    "            \n",
    "    return tc, vc, bc\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_counts_RAW, vocab_counts_RAW, bigram_counts_RAW = unigram_bigram_counts(emoji_targets, tweets_and_labels, term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_counts, vocab_counts, bigram_counts = prune_counts(target_counts_RAW, vocab_counts_RAW, bigram_counts_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 27523 2473\n"
     ]
    }
   ],
   "source": [
    "print(target_counts['UNK'], vocab_counts['UNK'], bigram_counts['UNK', 'UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_count_sum = sum(target_counts.values())\n",
    "vocab_count_sum = sum(vocab_counts.values())\n",
    "bigram_count_sum = sum(bigram_counts.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_v2i, target_i2v = unigram_index_maps(target_counts)\n",
    "vocab_v2i, vocab_i2v = unigram_index_maps(vocab_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmi_params = {\n",
    "    'tc'    : target_counts,\n",
    "    'vc'    : vocab_counts,\n",
    "    'bc'    : bigram_counts,\n",
    "    'tcs'   : target_count_sum,\n",
    "    'vcs'   : vocab_count_sum,\n",
    "    'bcs'   : bigram_count_sum,\n",
    "    'tv2i'  : target_v2i,\n",
    "    'ti2v'  : target_i2v,\n",
    "    'vv2i'  : vocab_v2i,\n",
    "    'vi2v'  : vocab_i2v\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_matrix, pmi_samples = pmi_matrix(pmi_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_pmi_matrix(pmi, t_i2v, t_v2i, v_i2v, v_v2i):\n",
    "    '''\n",
    "        pmi_matrix : the PMI matrix\n",
    "        i2v : target_v2i\n",
    "        v2i : target_i2v\n",
    "        \n",
    "        Computes U, sigma, Vh using SVD\n",
    "        Computes W_svd = U dot sigma^0.5\n",
    "        Normalizes W_svd:\n",
    "            rendering cosine similarity equivalent to dot product\n",
    "            \n",
    "    '''\n",
    "    # Factorize the PMI matrix\n",
    "    # k : number of singular values and vectors to compute\n",
    "    U, sigma, Vh = svds(pmi, k=150)\n",
    "    sigma_p = sigma**0.5\n",
    "    \n",
    "    # compute W_svd\n",
    "    W_svd = U*sigma_p\n",
    "    \n",
    "    # Normalize the vectors to enable computing cosine similarity    \n",
    "    norms = np.sqrt(np.sum(np.square(W_svd), axis=1, keepdims=True))\n",
    "    W_svd_n = U / np.maximum(norms, 1e-7)\n",
    "    \n",
    "    sample_emojis = list(t_v2i.keys())\n",
    "    k = 5\n",
    "    for x in sample_emojis:\n",
    "        '''\n",
    "            Cosine similarity for this unigram against all others\n",
    "        '''\n",
    "        dd = np.dot(W_svd_n, W_svd_n[t_v2i[x]])\n",
    "          \n",
    "        s = ''\n",
    "        # Get the list of nearest neighbor descriptions.\n",
    "        for i in np.argsort(dd)[-(k+1):]:\n",
    "            s += '(%s, %f) ' % (t_i2v[i], dd[i])\n",
    "        print('%s, %d\\n %s' % (x, target_counts[x], s))\n",
    "        print()\n",
    "        print('-' * 20)\n",
    "        print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize_pmi_matrix(pmi_matrix, target_i2v, target_v2i, vocab_i2v, vocab_v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pmi_matrix.count_nonzero())\n",
    "print()\n",
    "print(pmi_matrix.shape)\n",
    "print()\n",
    "print(pformat(pmi_samples.most_common()[:1000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nEXAMPLE GOOD RESULTS\\n\\nPositive correlations:\\n\\n(('ðŸ™', 'ðŸ˜¥'), 3.6183627184387683)\\n('ðŸ˜¯', 'ðŸ˜®'), 3.066208954490881)\\n(('ðŸœ', 'ðŸ±'), 3.463074333065022)\\n(('ðŸ˜œ', 'ðŸ˜'), 3.619660577154368)\\n(('ðŸ”¥', 'ðŸ’µ'), 3.6269598796359794)\\n(('ðŸº', 'ðŸ˜'), 3.5326492001647383)\\n(('ðŸº', 'ðŸ·'), 5.924267709047791)\\n\\nNegative correlations:\\n\\n(('ðŸ˜', 'ðŸ’”'), -2.038606008309401)\\n(('ðŸ˜˜', 'hate'), -3.398034123371635))\\n(('ðŸŽ¶', 'ðŸ˜©'), -3.481518571565408)\\n\\n\""
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "EXAMPLE GOOD RESULTS\n",
    "\n",
    "Positive correlations:\n",
    "\n",
    "(('ðŸ™', 'ðŸ˜¥'), 3.6183627184387683)\n",
    "('ðŸ˜¯', 'ðŸ˜®'), 3.066208954490881)\n",
    "(('ðŸœ', 'ðŸ±'), 3.463074333065022)\n",
    "(('ðŸ˜œ', 'ðŸ˜'), 3.619660577154368)\n",
    "(('ðŸ”¥', 'ðŸ’µ'), 3.6269598796359794)\n",
    "(('ðŸº', 'ðŸ˜'), 3.5326492001647383)\n",
    "(('ðŸº', 'ðŸ·'), 5.924267709047791)\n",
    "\n",
    "Negative correlations:\n",
    "\n",
    "(('ðŸ˜', 'ðŸ’”'), -2.038606008309401)\n",
    "(('ðŸ˜˜', 'hate'), -3.398034123371635))\n",
    "(('ðŸŽ¶', 'ðŸ˜©'), -3.481518571565408)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
