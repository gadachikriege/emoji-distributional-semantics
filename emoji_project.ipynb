{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import Counter\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> \n",
    "\n",
    "The following methods are only used to clean and save the original data and should not be run\n",
    "    \n",
    "    <b>The cleaned data can be found in emoji_datasets/all_data.csv</b>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path) as fp:\n",
    "        result = []\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            line = line.split(',',1)\n",
    "            if len(line) == 2:\n",
    "                clean_row1 = line[1].translate(translator)\n",
    "                clean_row2 = clean_row1.replace(chr(8220),'')\n",
    "                clean_row3 = clean_row2.replace(chr(8221),'')\n",
    "                line = [line[0], clean_row3]\n",
    "                value = np.array([line[0], line[1]])\n",
    "                result.append(value)\n",
    "            line = fp.readline()\n",
    "        return pd.DataFrame(np.array(result, dtype='object'))\n",
    "    \n",
    "def extract_emojis(example):\n",
    "    return (' '.join(c for c in example if c in emoji.UNICODE_EMOJI)).split()\n",
    "\n",
    "def prune_dataset_emojis(data):\n",
    "    result = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for i,row in enumerate(data):\n",
    "        try:\n",
    "            if extract_emojis(row[1]) != []:\n",
    "                clean_row1 = row[1].translate(translator)\n",
    "                clean_row2 = clean_row1.replace(chr(8220),'')\n",
    "                clean_row3 = clean_row2.replace(chr(8221),'')\n",
    "                new_row = np.array([row[0], clean_row3])\n",
    "                result.append(new_row)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return pd.DataFrame(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and write to CSV\n",
    "\n",
    "train_data_raw = pd.read_csv('emoji_datasets/data_train.csv', header=None, encoding='utf-8')\n",
    "\n",
    "test_data_raw = load_test_data('emoji_datasets/data_test.txt')\n",
    "\n",
    "train_data_clean = prune_dataset_emojis(train_data_raw.values)\n",
    "test_data_clean = prune_dataset_emojis(test_data_raw.values)\n",
    "\n",
    "all_data_clean_np = np.vstack((train_data_clean.values, test_data_clean.values))\n",
    "np.random.shuffle(all_data_clean_np)\n",
    "all_data_clean = pd.DataFrame(all_data_clean_np)\n",
    "\n",
    "all_data_clean.to_csv('emoji_datasets/all_data.csv', header=None, index=False, encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> END : clean data </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_labels_RAW = pd.read_csv('emoji_datasets/all_data.csv', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_emojis(example):\n",
    "    result = []\n",
    "    ptr = 0\n",
    "    for i,c in enumerate(example):\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            split = example[ptr:i]\n",
    "            if split != '':\n",
    "                result.append(split)\n",
    "                result.append(c)\n",
    "            else:\n",
    "                result.append(c)\n",
    "            ptr = i+1\n",
    "    return result\n",
    "\n",
    "def preprocess(data):\n",
    "    labels = list(data[:,0])\n",
    "    tweets = list(data[:,1])\n",
    "    result = []\n",
    "    tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    for i,twt in enumerate(tweets):\n",
    "        clean_tokens = []\n",
    "        tokens = tweet_tokenizer.tokenize(twt)\n",
    "        for j,tk in enumerate(tokens):\n",
    "            tk = tk.lower()\n",
    "            sep = separate_emojis(tk)\n",
    "            if sep != []:\n",
    "                clean_tokens = clean_tokens + sep\n",
    "            else:\n",
    "                clean_tokens.append(tk)\n",
    "        result.append((labels[i], clean_tokens))\n",
    "    return result\n",
    "\n",
    "def find_all_emojis(data):\n",
    "    emoji_dict = defaultdict(int)\n",
    "    for twt in data:\n",
    "        for word in twt[1]:\n",
    "            if word in emoji.UNICODE_EMOJI:\n",
    "                emoji_dict[word] += 1\n",
    "    return emoji_dict\n",
    "\n",
    "def term_context_matrix(targets, data):\n",
    "    tc_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for twt in data:\n",
    "        for w1 in targets:\n",
    "            if w1 in twt[1]:\n",
    "                for w2 in twt[1]:\n",
    "                    tc_matrix[w1][w2]+=1\n",
    "    return tc_matrix\n",
    "\n",
    "def vocab_map(dd):\n",
    "    vocab = {}\n",
    "    vocab_id = 0\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if k2 not in vocab.keys():\n",
    "                vocab[k2] = vocab_id\n",
    "                vocab_id += 1\n",
    "    return vocab\n",
    "\n",
    "def term_to_int_dd(dd):\n",
    "    num_rows = len(dd.keys())\n",
    "    data = defaultdict(int)\n",
    "    vocab_dict = vocab_map(dd)\n",
    "    for i,r in enumerate(dd.keys()):\n",
    "        for j,c in enumerate(dd[r].keys()):\n",
    "            data[i,vocab_dict[c]] = dd[r][c]\n",
    "            \n",
    "    return data\n",
    "\n",
    "def term_to_sparse(dd):\n",
    "    dd_int = term_to_int_dd(dd)\n",
    "    vs = [v for (i,j), v in dd_int.items()]\n",
    "    ii = [i for (i,j), v in dd_int.items()]\n",
    "    jj = [j for (i,j), v in dd_int.items()]\n",
    "    matrix = coo_matrix((vs, (ii, jj)))\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_labels = preprocess(tweets_and_labels_RAW.values)\n",
    "emoji_counts = find_all_emojis(tweets_and_labels)\n",
    "emoji_targets = list(emoji_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = term_context_matrix(emoji_targets, tweets_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_coo_matrix = term_to_sparse(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_bigram_counts(targets, tweets, term_matrix):\n",
    "    target_counter = Counter()\n",
    "    vocab_counter = Counter()\n",
    "    bigram_counter = Counter()\n",
    "    for twt in tweets:\n",
    "        for word in twt[1]:\n",
    "            if word in targets:\n",
    "                target_counter[word] += 1\n",
    "            vocab_counter[word] += 1\n",
    "    \n",
    "    for emoji in term_matrix.keys():\n",
    "        for word in term_matrix[emoji].keys():\n",
    "            bigram_counter[(emoji,word)] = term_matrix[emoji][word]\n",
    "            \n",
    "    return target_counter, vocab_counter, bigram_counter\n",
    "\n",
    "\n",
    "def unigram_index_maps(counter):\n",
    "    val_to_index, index_to_val = {}, {}\n",
    "    for i, x in enumerate(counter.keys()):\n",
    "        val_to_index[x] = i\n",
    "        index_to_val[i] = x\n",
    "    return val_to_index, index_to_val\n",
    "\n",
    "\n",
    "def pmi_matrix(params):\n",
    "    '''\n",
    "        tc   : target_counts\n",
    "        vc   : vocab_counts\n",
    "        bc   : bigram_counts\n",
    "        \n",
    "        tcs  : target_count_sum\n",
    "        vcs  : vocab_count_sum\n",
    "        bcs  : bigram_count_sum\n",
    "        \n",
    "        tv2i : target_v2i  (target value to index)\n",
    "        ti2v : target_i2v  (index to target value)\n",
    "        vv2i : vocab_v2i   (vocab value to index)\n",
    "        vi2v : vocab_i2v   (index to vocab value)\n",
    "    '''\n",
    "    \n",
    "    tc   = params['tc']\n",
    "    vc   = params['vc']\n",
    "    bc   = params['bc']\n",
    "    tcs  = params['tcs']\n",
    "    vcs  = params['vcs']\n",
    "    bcs  = params['bcs']\n",
    "    tv2i = params['tv2i']\n",
    "    ti2v = params['ti2v']\n",
    "    vv2i = params['vv2i']\n",
    "    vi2v = params['vi2v']\n",
    "    \n",
    "    pmi_samples = Counter()\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for (x, y), n in bc.items():\n",
    "        rows.append(tv2i[x]) # target index\n",
    "        cols.append(vv2i[y]) # context index\n",
    "        data.append(log((n / bcs) / ((tc[x] / tcs) * (vc[y] / vcs))))\n",
    "        pmi_samples[(x, y)] = data[-1]\n",
    "        \n",
    "    pmi_matrix = csc_matrix((data, (rows, cols)))\n",
    "    \n",
    "    return pmi_matrix, pmi_samples \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts, vocab_counts, bigram_counts = get_unigram_bigram_counts(emoji_targets, tweets_and_labels, term_matrix)\n",
    "target_count_sum = sum(target_counts.values())\n",
    "vocab_count_sum = sum(vocab_counts.values())\n",
    "bigram_count_sum = sum(bigram_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_v2i, target_i2v = unigram_index_maps(target_counts)\n",
    "vocab_v2i, vocab_i2v = unigram_index_maps(vocab_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pmi_params = {\n",
    "    'tc'    : target_counts,\n",
    "    'vc'    : vocab_counts,\n",
    "    'bc'    : bigram_counts,\n",
    "    'tcs'   : target_count_sum,\n",
    "    'vcs'   : vocab_count_sum,\n",
    "    'bcs'   : bigram_count_sum,\n",
    "    'tv2i'  : target_v2i,\n",
    "    'ti2v'  : target_i2v,\n",
    "    'vv2i'  : vocab_v2i,\n",
    "    'vi2v'  : vocab_i2v\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_matrix, pmi_samples = pmi_matrix(pmi_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124009\n",
      "\n",
      "(606, 24726)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pmi_matrix.count_nonzero())\n",
    "print()\n",
    "print(pmi_matrix.shape)\n",
    "print()\n",
    "#print(pmi_samples.most_common()[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good results (('üòç', 'üíî'), -2.038606008309401)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
